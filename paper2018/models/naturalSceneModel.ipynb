{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:06:59.622903Z",
     "start_time": "2025-01-09T15:06:59.315350Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdfe1b95d125a3c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:06:59.706401Z",
     "start_time": "2025-01-09T15:06:59.687194Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishaan/miniconda3/envs/VisualSearch/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/ishaan/miniconda3/envs/VisualSearch/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /Users/ishaan/miniconda3/envs/VisualSearch/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <8E6D6BF5-9658-33B9-9D3C-DF587B2F99E7> /Users/ishaan/miniconda3/envs/VisualSearch/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.conv import Conv2d\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8701266525ccaa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:07:00.608001Z",
     "start_time": "2025-01-09T15:07:00.571202Z"
    }
   },
   "outputs": [],
   "source": [
    "class IVSN(nn.Module):\n",
    "  def __init__(self, model):\n",
    "      super(IVSN, self).__init__()\n",
    "      self.features = nn.Sequential(*list(model.children())[0][:30])\n",
    "      for param in self.features.parameters():\n",
    "        param.requires_grad_ = False\n",
    "\n",
    "  def forward(self, x):\n",
    "      x = self.features(x)\n",
    "      return x\n",
    "\n",
    "ConvSize, NumTemplates, Mylayer = 1, 512, 31\n",
    "TotalTrials, targetsize, stimulisize = 240, (640, 480), (1280, 1024)\n",
    "MMconv = Conv2d(NumTemplates, 1, kernel_size = (ConvSize, ConvSize), stride = (1, 1), padding = (1, 1))\n",
    "# 512 input channels, 1 output channel, 1x1 kernel, stride 1, padding 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95cca5f3d69f1568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:07:02.541253Z",
     "start_time": "2025-01-09T15:07:01.511131Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/ishaan/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IVSN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "model_vgg = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', weights=VGG16_Weights.DEFAULT)\n",
    "model_ivsn = IVSN(model_vgg)\n",
    "model_ivsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4550c8bd4d00f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:07:03.656941Z",
     "start_time": "2025-01-09T15:07:03.637792Z"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import naturalSceneDataset\n",
    "\n",
    "input_images = naturalSceneDataset.NaturalSceneDataset('data/naturaldesign/gt', 'data/naturaldesign/stimuli', 'data/naturaldesign/target', normalize_means=[0.485, 0.456, 0.406], normalize_stds=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c5c75a80836161",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:21:05.752847Z",
     "start_time": "2025-01-09T15:21:05.339007Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.imshow(input_images[0][0].permute(1, 2, 0))\n",
    "#plt.show()\n",
    "#plt.imshow(input_images[0][1].permute(1, 2, 0))\n",
    "#print(input_images[0][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaf9b9e291eee11c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:26:06.188509Z",
     "start_time": "2025-01-09T15:26:00.246417Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/naturaldesign/target/t001.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m         stimuli_img, target_img, bbox \u001b[38;5;241m=\u001b[39m \u001b[43minput_images\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m#Need 3 channels:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stimuli_img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/CS/Research/InvariantVisualSearchNetwork/paper2018/models/naturalSceneDataset.py:67\u001b[0m, in \u001b[0;36mNaturalSceneDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Load the target and context images\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     target_img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/t\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m03d\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     stimuli_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstimuli_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/img\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     69\u001b[0m     gt_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgt_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/gt\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# [3, 1024, 1280]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/VisualSearch/lib/python3.11/site-packages/PIL/Image.py:3227\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3224\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3227\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3228\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/naturaldesign/target/t001.jpg'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import trange\n",
    "from utils import naturalSearchProcesswithPath\n",
    "\n",
    "num_pics = len(input_images) #240 images\n",
    "IVSN_attention_map, scanpath = {}, {}\n",
    "IVSN_res = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for id in range(1):\n",
    "        stimuli_img, target_img, bbox = input_images[id]\n",
    "        #Need 3 channels:\n",
    "        if stimuli_img.shape[0] == 1:\n",
    "            stimuli_img = torch.cat((stimuli_img, stimuli_img, stimuli_img), 0)\n",
    "        if target_img.shape[0] == 1:\n",
    "            target_img = torch.cat((target_img, target_img, target_img), 0)\n",
    "        \n",
    "        \n",
    "        stimuli_batch = stimuli_img.unsqueeze(0)\n",
    "        target_batch = target_img.unsqueeze(0) \n",
    "        \n",
    "        # Get the output from the model after going through all 30 layers\n",
    "        stimuli_output = model_ivsn(stimuli_batch)\n",
    "        target_output = model_ivsn(target_batch)\n",
    "        \n",
    "        # Update MMconv weights with target output\n",
    "        MMconv.weight = torch.nn.Parameter(target_output)\n",
    "        \n",
    "        # Get the attention map by applying MMconv to stimuli_output\n",
    "        attention_IVSN = MMconv(stimuli_output) \n",
    "        attention_IVSN = attention_IVSN.squeeze(0)\n",
    "                \n",
    "        # Normalize the attention map\n",
    "        mask_IVSN = transforms.Resize((stimuli_img.shape[1], stimuli_img.shape[2]))(attention_IVSN)\n",
    "        mask_IVSN = mask_IVSN.squeeze(0)\n",
    "        mask_IVSN = torch.divide(mask_IVSN, mask_IVSN.max())\n",
    "                        \n",
    "        IVSN_attention_map[id] = mask_IVSN.clone().detach() #create deepcopy\n",
    "        \n",
    "        IVSN_num, path = naturalSearchProcesswithPath(mask_IVSN, bbox)\n",
    "        scanpath[id] = path\n",
    "        IVSN_res.append(IVSN_num)\n",
    "\n",
    "print(IVSN_res)\n",
    "\n",
    "plt.imshow(IVSN_attention_map[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654bc02fa9a861a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Might need to split up the image into a bunch of images and concatenate -- later"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VisualSearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
